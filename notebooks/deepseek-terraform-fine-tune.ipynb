{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10874165,"sourceType":"datasetVersion","datasetId":6756400},{"sourceId":10894916,"sourceType":"datasetVersion","datasetId":6770708},{"sourceId":10900574,"sourceType":"datasetVersion","datasetId":6774677},{"sourceId":225259848,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# !pip install torch transformers datasets accelerate peft bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T20:08:07.963473Z","iopub.execute_input":"2025-03-02T20:08:07.963771Z","iopub.status.idle":"2025-03-02T20:08:07.967871Z","shell.execute_reply.started":"2025-03-02T20:08:07.963740Z","shell.execute_reply":"2025-03-02T20:08:07.967000Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Pipeline","metadata":{}},{"cell_type":"code","source":"!rm -r /kaggle/working/wandb/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:08:39.657463Z","iopub.execute_input":"2025-03-02T22:08:39.657793Z","iopub.status.idle":"2025-03-02T22:08:39.967385Z","shell.execute_reply.started":"2025-03-02T22:08:39.657767Z","shell.execute_reply":"2025-03-02T22:08:39.966607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_apikey\")\n\nwandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:08:41.142456Z","iopub.execute_input":"2025-03-02T22:08:41.142801Z","iopub.status.idle":"2025-03-02T22:08:41.309783Z","shell.execute_reply.started":"2025-03-02T22:08:41.142763Z","shell.execute_reply":"2025-03-02T22:08:41.309081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nimport os\n\n# Define the model name\nMODEL_NAME = \"deepseek-ai/deepseek-coder-1.3b-base\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token # add a pad token\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load and preprocess dataset\nTRAINING_DATA= \"/kaggle/input/terraform-aws-custom-train/terraform_aws_docs_training_data2 (1).jsonl\"\nTEST_DATA =\"/kaggle/input/validated-terraform/validated_terraform_data.jsonl\"\ndataset =  load_dataset(\"json\", data_files={\"train\": TRAINING_DATA, \"test\": TEST_DATA})\n\n\n# Format dataset for training\ndef format_prompt(example):\n    return {\"input\": f\"### Instruction:\\n{example['prompt']}\\n\\n### Response:\\n{example['completion']}\"}\n\ndataset = dataset.map(format_prompt)\n\n# tokenize the dataset\ndef tokenize_function(example):\n    \"\"\"\n    Tokenizes the 'input' text in the example using the global tokenizer.\n    Adds padding and truncation to handle variable sequence lengths.\n\n    Args:\n        example (dict): A dictionary containing an 'input' key with text data.\n\n    Returns:\n        dict: A dictionary containing tokenized data.\n    \"\"\"\n    max_length = 151 # set the maximum length here\n    result = tokenizer(\n        example['input'],\n        padding='max_length',  # Pad to max_length\n        truncation=True,  # Truncate to max_length\n        max_length=max_length # explicitly set max length\n    )\n    result[\"labels\"] = result[\"input_ids\"].copy()  # labels are the same as input_ids for causal language models\n    return result\n\ntokenized_datasets = dataset.map(tokenize_function, remove_columns=['input'])\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8, lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"]\n)\n\n# Apply LoRA to model\nmodel = get_peft_model(model, lora_config)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    warmup_steps=100,\n    num_train_epochs=1.25,\n    logging_steps=5,\n    save_strategy=\"epoch\", # change save strategy\n    output_dir=\"./fine_tuned_deepseek\",\n    remove_unused_columns=True # change to true\n)\n\n#create the data collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=151) #mlm is false since we aren't using masked language modeling, pad to multiple of 151 since that was the expected sequence length\n\n# Train the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    #eval_dataset=tokenized_datasets['test'],\n    data_collator=data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:16:36.858753Z","iopub.execute_input":"2025-03-02T22:16:36.859097Z","iopub.status.idle":"2025-03-02T22:16:42.084457Z","shell.execute_reply.started":"2025-03-02T22:16:36.859068Z","shell.execute_reply":"2025-03-02T22:16:42.083662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Before Fine-tuning","metadata":{}},{"cell_type":"code","source":"prompt = \"Create a Terraform template with AWS Lambda function which reads S3\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate response\noutput = model.generate(**inputs, max_length=1000)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:16:46.798032Z","iopub.execute_input":"2025-03-02T22:16:46.798366Z","iopub.status.idle":"2025-03-02T22:17:14.796500Z","shell.execute_reply.started":"2025-03-02T22:16:46.798340Z","shell.execute_reply":"2025-03-02T22:17:14.795428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:10:26.898299Z","iopub.execute_input":"2025-03-02T22:10:26.898598Z","iopub.status.idle":"2025-03-02T22:13:10.651535Z","shell.execute_reply.started":"2025-03-02T22:10:26.898574Z","shell.execute_reply":"2025-03-02T22:13:10.650872Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# After Fine Tuning","metadata":{}},{"cell_type":"code","source":"prompt = \"Create a Terraform template with an AWS Lambda function which reads S3 bucket\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate response\noutput = model.generate(**inputs, max_length=1000)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:14:23.187829Z","iopub.execute_input":"2025-03-02T22:14:23.188125Z","iopub.status.idle":"2025-03-02T22:14:54.696098Z","shell.execute_reply.started":"2025-03-02T22:14:23.188102Z","shell.execute_reply":"2025-03-02T22:14:54.695253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example input\nfine_tuned_model = model\nprompt = \"Create Terraform template with AWS db cluster snapshot resource\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# Generate response\noutput = fine_tuned_model.generate(**inputs, max_length=1000)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:13:55.813870Z","iopub.execute_input":"2025-03-02T22:13:55.814220Z","iopub.status.idle":"2025-03-02T22:13:58.842043Z","shell.execute_reply.started":"2025-03-02T22:13:55.814188Z","shell.execute_reply":"2025-03-02T22:13:58.841146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the trained model","metadata":{}},{"cell_type":"code","source":"!rm -r /kaggle/working/fine_tuned_deepseek/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:16:07.708425Z","iopub.execute_input":"2025-03-02T22:16:07.708727Z","iopub.status.idle":"2025-03-02T22:16:08.033680Z","shell.execute_reply.started":"2025-03-02T22:16:07.708703Z","shell.execute_reply":"2025-03-02T22:16:08.032811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_deepseek\")\ntokenizer.save_pretrained(\"./fine_tuned_deepseek\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:16:09.452873Z","iopub.execute_input":"2025-03-02T22:16:09.453160Z","iopub.status.idle":"2025-03-02T22:16:09.636537Z","shell.execute_reply.started":"2025-03-02T22:16:09.453137Z","shell.execute_reply":"2025-03-02T22:16:09.635734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BEST_MODEL_DIR = \"./fine_tuned_deepseek\"\n\nmodel = AutoModelForCausalLM.from_pretrained(BEST_MODEL_DIR, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:16:12.702966Z","iopub.execute_input":"2025-03-02T22:16:12.703371Z","iopub.status.idle":"2025-03-02T22:16:18.063525Z","shell.execute_reply.started":"2025-03-02T22:16:12.703335Z","shell.execute_reply":"2025-03-02T22:16:18.062494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r best_model.zip /kaggle/working/fine_tuned_deepseek","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T22:16:18.862292Z","iopub.execute_input":"2025-03-02T22:16:18.862631Z","iopub.status.idle":"2025-03-02T22:16:19.575956Z","shell.execute_reply.started":"2025-03-02T22:16:18.862602Z","shell.execute_reply":"2025-03-02T22:16:19.575211Z"}},"outputs":[],"execution_count":null}]}